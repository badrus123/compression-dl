{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1. JST _ MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badrus123/compression-dl/blob/master/1_JST___MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuwBcu7g9RnO",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://anvaqta.id/headerai.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbRdBGQ8bHs7",
        "colab_type": "text"
      },
      "source": [
        "# Bagian Baru"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GwIMoQeVCkPD"
      },
      "source": [
        "---\n",
        "# Neural Network\n",
        "\n",
        "Neural Network is almost always ilustrated as having the same computational work as in Human Brain, especially considering its main component: ***Neuron***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1HVAbBSD4_ig"
      },
      "source": [
        "---\n",
        "## a. Neuron\n",
        "Neuron Human  |  Neural Network Neuron\n",
        "-- | --\n",
        "![neuron](http://cs231n.github.io/assets/nn1/neuron.png) | ![neuron](http://cs231n.github.io/assets/nn1/neuron_model.jpeg)\n",
        "\n",
        "Based on that concept and analogy, we can implement **forward pass function** of a neuron in `Python` as follow:\n",
        "```python\n",
        "def forward(self, inputs):\n",
        "    \"\"\" assume inputs and weights are 1-D numpy arrays and bias is a number \"\"\"\n",
        "    cell_body_sum = np.sum(inputs * self.weights) + self.bias   # affine function\n",
        "    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum))        # sigmoid activation function\n",
        "    return firing_rate\n",
        "```\n",
        "\n",
        "The basic computation inside a neuron is a weighted sum of its input. Neural Network then learn by modifying the weights in each neuron to minimize the output error.\n",
        "\n",
        "To simplify the computation, all neurons are grouped in stacks called **layers**. Thus, all weights of neurons in a layer can be formed as a matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iHc_y5DwCkQd"
      },
      "source": [
        "---\n",
        "## b. Single Layer Perceptron\n",
        "Seperti yang sudah kalian pelajari, di dalam Jaringan Saraf Tiruan, pengetahuan yang dipelajari disimpan dalam bobot masing-masing neuron. Untuk mempermudah perhitungan, neuron dikumpulkan ke dalam kelompok-kelompok yang disebut layer sehingga bobot-bobot neuron dalam satu layer dapat dibentuk sebagai sebuah matrik.\n",
        "\n",
        "Sebagai contoh, misalkan kita hanya menggunakan satu layer saja, atau yang biasa disebut sebagai ***Single-Layer Perceptron***, maka jaringan akan terlihat seperti ilustrasi di bawah.\n",
        "\n",
        "![onelayer](https://image.ibb.co/fjR3oz/onelayer.png) \n",
        "\n",
        "\n",
        "Dari ilustrasi dapat dilihat bahwa Jaringan Saraf Tiruan dengan hanya satu layer tidak lain adalah fungsi regresi linier. \n",
        "\n",
        "Sudah kalian ketahui juga bahwa JST satu layer dengan pelatihan *gradient descent* yang terjadi adalah\n",
        "* melakukan proses maju yang mengalikan input dengan bobot,\n",
        "* kemudian melakukan proses mundur untuk mendapatkan gradien input dan gradien bobot\n",
        "\n",
        "Jika kita tuliskan secara singkat, kode Single Layer Perceptron dalam bahasa python, kita hanya membutuhkan beberapa baris (menggunakan tensorflow)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nptPz2tA6x9P",
        "colab_type": "code",
        "outputId": "ec570afc-fd61-46b8-bfd4-ed89bcc95cf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 46kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 42.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 45.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.17.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (41.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-2.0.0 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PUbQT6YLQNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def single_layer_model(neuron, height, width, dim):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape = (height,width,dim)),\n",
        "        tf.keras.layers.Dense(neuron),\n",
        "        tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azXQTYgwLQNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = single_layer_model(2048,28,28,1)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',     \n",
        "    optimizer='sgd',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYM-AaiDPeGw",
        "colab_type": "code",
        "outputId": "81696454-14a2-415e-ece7-7a26f5a76ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2048)              1607680   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,609,729\n",
            "Trainable params: 1,609,729\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpR40fLhLYPs",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1pL27I4oCkSY"
      },
      "source": [
        "---\n",
        "## c. Multi Layer Perceptron\n",
        "Dalam penggunaan Jaringan Saraf Tiruan, kita dapat menumpuk layer-layer neuron menjadi sebuah arsitektur baru yang disebut sebagai *Multi-layer Perceptron* (MLP). Layer-layer yang berada di antara layer input dan layer output disebut sebagai *hidden layer*\n",
        "\n",
        "MLP dengan 2 layer neuron biasa disebut sebagai *2-layer neural net* atau *1-hidden-layer neural net*.\n",
        "begitu pula untuk MLP dengan 3 layer biasa disebut sebagai *3-layer neural net* atau *2-hidden-layer neural net*. Seperti ilustrasi berikut\n",
        "\n",
        "*2-layer NN* | *3-layer NN*\n",
        "- | -\n",
        "![2layerNN](https://image.ibb.co/dHnnFe/2layerNN.png) | ![3layerNN](https://image.ibb.co/iH18MK/3layerNN.png)\n",
        "\n",
        "Di sini dapat kalian perhatikan bahwa sebenarnya, selama pelatihan, MLP mengulang-ulang proses yang dilakukan Single Layer Perceptron, baik pada proses maju dan mundur. Kita bisa saja menuliskan proses pelatihan yang spesifik untuk setiap model kedalaman jaringan.\n",
        "\n",
        "Namun alangkah mudahnya jika kita bisa menuliskannya dalam suatu bentuk fungsi sehingga kita menjadi mudah menambah atau mengurangi jumlah kedalaman model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAd0d8DcLQNJ",
        "colab_type": "code",
        "outputId": "2267fc01-5a1c-4c88-8dfd-628083e8db2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
        "    tf.keras.layers.Dense(512),\n",
        "    tf.keras.layers.Dense(512),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 665,089\n",
            "Trainable params: 665,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_1KJ9VAMWrr",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "385beyHA4_lb"
      },
      "source": [
        "---\n",
        "## d. Hidden Neuron\n",
        "Now let's investigate the hidden neuron hyperparameter\n",
        "\n",
        "In theory, more hidden neuron is better, more layer is also better.\n",
        "But too much layer and neuron is wasteful on your resource since its heavier and much longer to train\n",
        "\n",
        "Let's see if we train 20 hidden neuron using learning rate=0.01\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fIGH2uioCkcY"
      },
      "source": [
        "The main problem with too many hidden neuron or hidden layer is overfitting\n",
        "\n",
        "<p align=\"center\"><img src=\"http://cs231n.github.io/assets/nn1/layer_sizes.jpeg\" width=\"750\" ></p>\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HYh-V6raicR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## e. Bobot dan Bias\n",
        "*2-layer NN* | *3-layer NN*\n",
        "- | -\n",
        "![2layerNN](https://image.ibb.co/dHnnFe/2layerNN.png) | ![3layerNN](https://image.ibb.co/iH18MK/3layerNN.png)\n",
        "\n",
        "Setiap neuron pada MLP saling berhubungan yang ditandai dengan tanda panah pada gambar diatas. Tiap koneksi memiliki weight yang nantinya nilai dari tiap weight akan berbeda-beda.\n",
        "Hidden layer dan output layer memiliki tambahan “input” yang biasa disebut dengan bias (Tidak disebutkan pada gambar diatas).\n",
        "Sehingga pada arsitektur pertama terdapat 3x4 weight + 4 bias dan 4x2 weight + 2 bias. Total adalah 26 parameter yang pada proses training akan mengalami perubahan untuk mendapatkan hasil yang terbaik. Sedangkan pada arsitektur kedua terdapat 46 parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njNfZ7V3bsMU",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## f. Fungsi Aktivasi\n",
        "Fungsi aktivasi berguna untuk menentukan neuron mana yang aktif yang selanjutnya akan diteruskan ke layer berikutnya. Pada input tidak terdapat fungsi aktivasi, sedangkan pada output layer memiliki fungsi aktivasi yang berbeda-beda tergantung permasalahan yang diselesaikan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-YTxp42cIp0",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Melatih Model Jaringan Syaraf Tiruan\n",
        "Pada Supervised Learning menggunakan Neural Network, pada umumnya Learning terdiri dari 2 tahap, yaitu training dan evaluation. Namun kadang terdapat tahap tambahan yaitu testing, namun sifatnya tidak wajib.<br><br>\n",
        "Pada tahap training setiap weight dan bias pada tiap neuron akan diupdate terus menerus hingga output yang dihasilkan sesuai dengan harapan. Pada tiap iterasi akan dilakukan proses evaluation yang biasanya digunakan untuk menentukan kapan harus menghentikan proses training (stopping point). <br><br>\n",
        "Secara umum terdapat 2 proses training pada JST, yaitu **Forward Pass** dan **Backward Pass**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTmBRRiWcvw_",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## a. Forward Pass\n",
        "Forward pass atau biasa juga disebut forward propagation adalah proses dimana kita membawa data pada input melewati tiap neuron pada hidden layer sampai kepada output layer yang nanti akan dihitung errornya\n",
        "\n",
        "![alt text](https://miro.medium.com/max/438/1*gYoJzQEMRFafH8AAt6KRNQ.jpeg)\n",
        "\n",
        "Persamaan diatas adalah contoh forward pass pada arsitektur pertama (lihat gambar arsitektur diatas) yang menggunakan ReLU sebagai activation function. Dimana i adalah node pada input layer (3 node input), j adalah node pada hidden layer sedangkan h adalah output dari node pada hidden layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5wZltkXZ3fl",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## b. Backward Pass\n",
        "Error yang kita dapat pada forward pass akan digunakan untuk mengupdate setiap weight dan bias dengan learning rate tertentu.\n",
        "Kedua proses diatas akan dilakukan berulang-ulang sampai didapatkan nilai weight dan bias yang dapat memberikan nilai error sekecil mungkin pada output layer (pada saat forward pass)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTad2DS5dFPy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "![alt text](https://miro.medium.com/max/983/1*bsjmNlxtISCrsYg1Jy7X3g.jpeg)\n",
        "\n",
        "Pada gambar diatas, Forward Pass ditandai dengan tanda panah berwarna biru, sedangkan backward pass ditandai dengan tanda panah berwarna merah<br><br>\n",
        "\n",
        "Dalam Supervised Learning, training data terdiri dari input dan output/target. Pada saat forward pass, input akan di-”propagate” menuju output layer dan hasil prediksi output akan dibandingakan dengan target dengan menggunakan sebuah fungsi yang biasa disebut dengan Loss Function.<br><br>\n",
        "\n",
        "Loss function berperan penting, karena loss function bertugas untuk menentukan seberapa besar perubahan yang harus kita lakukan pada bobot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzBiYcn7eXUb",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## c. Backpropagation\n",
        "Backpropagation merupakan algoritma untuk mengupdate bobot dan bias pada jaringan syaraf tiruan. Simpelnya algoritma ini akan mengupdate bobot berdasarkan loss yang didapat saat forward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0xj0nAqeINq",
        "colab_type": "text"
      },
      "source": [
        "Tahap pertama yaitu hitung gradient dari loss function terhadap semua parameter yang ada dengan cara mencari partial derivative (turunan parsial) dari fungsi tersebut. Disini kita bisa menggunakan metode Chain Rule. Untuk yang masih bingung apa itu gradient, mungkin ilustrasi dibawah ini bisa membantu.\n",
        "![alt text](https://miro.medium.com/max/330/1*dunz7C-rCGs1zajsYWI45w.gif)\n",
        "\n",
        "Update semua parameter (weight dan bias) menggunakan ***Stochastic Gradient Descent (SGD)*** dengan mengurangi atau menambahkan nilai weight lama dengan “sebagian” (learning rate) dari nilai gradient yang sudah kita dapat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw2iE8pAe_JX",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://media3.giphy.com/media/4LiMmbAcvgTQs/200.webp?cid=790b7611166b897e023116721f89d7d0d7319d321fe6becd&rid=200.webp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra-jsWOa2uRZ",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://scontent-lga3-1.cdninstagram.com/vp/dab1b53a1164891363449016f262a7a2/5E507CD2/t51.2885-15/e35/73169329_455297468217907_4492185464377407939_n.jpg?_nc_ht=scontent-lga3-1.cdninstagram.com&_nc_cat=107&dl=1)"
      ]
    }
  ]
}